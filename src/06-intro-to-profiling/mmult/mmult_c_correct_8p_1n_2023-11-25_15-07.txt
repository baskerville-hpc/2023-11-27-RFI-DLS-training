Command:        /bask/homes/w/wongj/yearwoog-baskerville/wongj/forge/mmult/mmult_c_correct 3072
Resources:      1 node (72 physical, 144 logical cores per node)
Memory:         504 GiB per node
Tasks:          8 processes
Machine:        bask-pg0308u23a.cluster.baskerville.ac.uk
Architecture:   x86_64
CPU Family:     icelake-x
Start time:     Sat Nov 25 15:07:04 2023
Total time:     78 seconds (about 1 minutes)
Full path:      /bask/projects/y/yearwoog-baskerville/wongj/forge/mmult

Summary: mmult_c_correct is Compute-bound in this configuration
Compute:                                     69.3% |======|
MPI:                                         30.7% |==|
I/O:                                      &lt;0.1% ||
This application run was Compute-bound (based on main thread activity). A breakdown of this time and advice for investigating further is in the CPU section below.

CPU:
A breakdown of the 69.3% CPU time:
Scalar numeric ops:                          13.9% ||
Vector numeric ops:                           0.0% |
Memory accesses:                             85.9% |========|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.
No time is spent in vectorized instructions. Check the compiler's vectorization advice to see why key loops could not be vectorized.

MPI:
A breakdown of the 30.7% MPI time:
Time in collective calls:                    18.1% |=|
Time in point-to-point calls:                81.9% |=======|
Effective process collective rate:            0.00 bytes/s
Effective process point-to-point rate:        38.3 MB/s

I/O:
A breakdown of the <0.1% I/O time:
Time in reads:                                0.0% |
Time in writes:                             100.0% |=========|
Effective process read rate:                  0.00 bytes/s
Effective process write rate:                 1.45 GB/s
Most of the time is spent in write operations with a high effective transfer rate. It may be possible to achieve faster effective transfer rates using asynchronous file operations.

Threads:
A breakdown of how multiple threads were used:
Computation:                                100.0% |=========|
Synchronization:                              0.0% |
Physical core utilization:                   11.1% ||
System load:                                158.4% |===============|
Physical core utilization is low. Try increasing the number of threads or processes to improve performance.
The system load is high - multiple threads may be sharing one core. Check your thread affinity settings.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                     279 MiB
Peak process memory usage:                     393 MiB
Peak node memory usage:                       5.0% ||
The peak node memory usage is very low. Running with fewer MPI processes and more data on each process may be more efficient.

Energy:
A breakdown of how the 9.27 Wh was used:
CPU:                                        100.0% |=========|
System:                                   not supported
Mean node power:                          not supported
Peak node power:                              0.00 W
The whole system energy has been calculated using the CPU energy usage.
System power metrics: Cray power not supported

